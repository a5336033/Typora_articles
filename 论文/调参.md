# 卷积神经网络图像分类技巧包

## 背景

图像分类取得的许多进展归功于训练程序的改进，如：数据增强、优化方法的变化。

然而在文献中，大多数改进要么在实现细节中被简要提及，要么仅在源代码中可见。

本文，检查了改进的集合，并通过消融实验研究它们对最终模型精度的影响。

***

## 内容

> 引入

自AlexNet问世，多种架构被提出，能够看到模型精确度稳步上升的趋势。然而准确率的提高不单单来自于准确率的提高。训练程序的改进，包括**损失函数的变化、数据预处理、优化方法**也发挥了重要的作用。

> 工作

测试了提高模型准确度、但几乎没有改变计算复杂度的训练程序和模型架构的集合。包括很多”tricks“，并在多个网络架构和数据集上进行评估，并报告它们对最终模型精度的影响。

概述：

1. 建立了baseline训练程序
2. 讨论了几个对于新硬件的有效训练非常有用的技巧
3. 回顾了ResNet的三个小模型架构调整，并提出了一个新的模型架构
4. 讨论四个额外的训练程序改进
5.  研究这些更精确的模型是否有助于迁移学习

---

### 训练程序baseline

![image-20221012160342081](../assets/调参/image-20221012160342081.png)

> Baseline训练过程

使用ResNet作为baseline。训练和验证之间的预处理是不同的。

训练中：

1. 随机抽样一副图像，解码成32位浮点原始像素值[0, 255]
2. 随机裁剪长宽比为[3/4,  4/3] 随机采样的矩阵区域和面积为[8%, 100%]随机采样的区域，调整裁剪区域为224 * 224的图像
3. 水平翻转（概率为0.5）
4. 根据[0.6, 1.4]的系数统一划分色调，饱和度和亮度
5. 从正态分布N (0, 0.1)中采样添加PCA噪声的系数
6. 正则化RGB通道，通过分别减去123.68，116.779, 103.939 并除以 58.393, 57.12,
   57.375

验证时：

	1. 将每个图像的短边调整为256像素，同时保持长宽比
 	2. 裁剪出中心的224*224区域，并正则化（类似于训练的RGB通道）
 	3. 验证时不执行任何的随机增强

卷积层和全连接层的权值都用Xavier算法初始化

NAG加速梯度下降用于训练

每个模型在8个Nvidia V100 gpu上训练120 epoch，总批大小为256。学习率初始化为0.1，在第30、60和90个周期除以10

注解：pipeline：流水线（数据读取、数据预处理、创建模型、评估模型结果、模型调参）

> 有效训练

硬件，尤其是CPU近些年的快速发展，许多与性能相关的权衡已经改变。例如：现在在训练中使用较低的数值精度和较大的批处理规模更有效。接下来将回顾各种不牺牲模型准确度实现低精度和大批量的技术训练方法。有些方法甚至可以提高准确率和训练速度。

> > Large-batch training

小批量SGD将多个样本分组到一个小批，以增加并行性并降低通信成本。但是，大批量可能会减慢训练速度。对于凸问题收敛速度随着批量大小的增加而降低。换句话说，对于相同的周期数，与小批次训练的模型相比，大批次训练的模型验证精度下降。（本文研究四种启发式方法，它们有助于扩大单台机器训练的批大小）

> > Linear scaling learning rate

在小批SGD中，梯度下降是一个随机过程，因为每个批中的样例是随机的。增加批的大小不会改变随机梯度的期望，但会减少其方差。换句话说，大批量减少了梯度中的噪声，所以我们可以增加学习率，沿着梯度方向的相反方向取得更大的变化。

Goyal等人报告称，在ResNet-50训练中，学习率随批次大小的线性增加在经验上是有效的。具体而言，在批量大小为256的情况下选择0.1作为初始学习率，那么当改变更大的批量大小为b时，将初始学习率提高到0.1 * (b/256)

> > Learning rate warmup(学习率预热)

在训练开始时，所有参数都是典型的随机值，因此远离最终解。使用较大的学习率可能会导致训练不稳定，所以在热身启发式中，我们在开始时使用一个小的学习率，然后在训练过程稳定后切换回初始学习率。

Goyal等提出了一种循序渐进的热身策略，将学习率从0线性增加到初始学习率。具体来说，假设我们将使用前m批(例如5个数据周期)进行热身，初始学习率为η，那么在第i批，1≤i≤m时，我们将学习率设置为iη/m。

> > Zero γ

ResNet网络由多个剩余块组成，每个块由几个卷积层组成。给定的输入x,假设块(x)是块中最后一层的输出，这个剩余的块然后输出x +块(x)。块的最后一层可以是批处理规范化（BN）层。
![image-20221012191602111](../assets/调参/image-20221012191602111.png)

> > No bias decay(无偏执衰退)

权重衰减可以用于所有可学习的参数，包括权重和偏执。

Jia等人指出，建议只将正则化应用于权重，以避免过拟合。无偏差衰减启发式遵循这一建议，它只将权值衰减应用于卷积层和全连接层中的权值。其他参数，包括BN层中的偏差和γ和β，都是非正则化的。

注意，LARS[4]提供了分层的自适应学习率，据报道，对于超大批处理大小(超过16K)非常有效。而在本文中，我们限制自己的方法足以单机训练，在这种情况下，批大小不超过2K往往导致良好的系统效率。



> Low-precision training

神经网络通常采用32位浮点(FP32)精度进行训练。也就是说，所有的数字都以FP32格式存储，算术运算的输入和输出也是FP32数字。然而，新的硬件可以增强算术逻辑单元，以满足较低精度的数据类型。例如，前面提到的Nvidia V100在FP32中提供14个TFLOPS，但在FP16中提供超过100个TFLOPS。英伟达的部分显卡针对FP16做了定制化优化，能够达到更快的计算速度，比如最新的显卡V100。

速度更快，精度可能降低，权衡方法

1. 将所有参数和激活存储在FP16中，并用FP16计算梯度，同时，所有参数在FP32中都有一个副本，用于参数更新。
2. 用一个标量乘以损失，更好地将梯度的范围对齐到FP16

注:TFLOPS等于每秒一万亿（=10^12）次的浮点运算

> 实验结果

![image-20221012192546013](../assets/调参/image-20221012192546013.png)

Table3:左边为叠加多种训练策略，右边为Baseline。

Table4：消融实验，batch size从256增加到1024会降低准确率，通过启发式叠加可以弥补差距。训练结束时从FP32切换到FP16不影响精度。

> 模型调整

对网络架构得微小调整。例如：改变特定卷积层的步幅。这样的调整通常几乎不会改变计算复杂度。但可能对模型精度有不可忽略的影响。

本节以ResNet为例研究模型调整的效果。

> > ResNet 架构

![image-20221012194458363](../assets/调参/image-20221012194458363.png)

> >修改后的架构

![image-20221012195023676](../assets/调参/image-20221012195023676.png)

图Figure左边为瓶颈结构，**减少了参数量，加深了网络层数**，1\*1卷积的妙用。减少卷积核的通道数，然后3\*3提取特征，最后1\*1提高通道数使其达到目标数量。a图右边将输入形状转换成

图b，3个3\*3的卷积替换了输入杆中的7\*7的卷积。减少了参数量。

图c，下采样块路劲B上的1\*1卷积忽略了3/4的输入特征映射，想修改它，使任何信息都不会被忽略 ，实践中，在卷积前添加一个步幅为2的2\*2平均池化层，及那个其步幅改为1。效果很好，计算成本影响很小，

注：卷积前后计算方式：

输入图片大小 W×W
Filter大小 F×F
步长 S
padding的[像素](https://so.csdn.net/so/search?q=像素&spm=1001.2101.3001.7020)数 P
输出图片大小为 N×N
那么：
N = floor[(W − F + 2P )/S+1]
**floor[.]表示向下取整**

> 训练改进

> > 余弦学习率衰减

学习率的调整是训练的关键。学习率热身之后，通常会从初始学习率的值稳步下降。广泛使用的策略是指数衰减学习率。eg：每30个epoch，学习率下降0.1；每2个epoch，学习率变为0.94。

余弦退火策略

1. 简化版本：通过遵循余弦函数将学习率从初始值降低到0。假设总批次数为T，则第T批次时，学习率ηt计算为:

![image-20221014103729556](../assets/调参/image-20221014103729556.png)

η为初始学习率，这种调度为"余弦"衰减。

![image-20221014104224792](../assets/调参/image-20221014104224792.png)

> > 标签平滑

图像分类网络的最后一层通常是一个完全连接的层，其隐藏大小等于标签的数量，用K表示，用于输出预测的置信度分数。给定一副图像，zi表示第i类的预测得分。这些分数可以通过softmax算子归一化得到预测概率。用q表示softmax算子q = softmax（z）的输出，类i，qi的概率可由以下公式计算：

![image-20221014104723505](../assets/调参/image-20221014104723505.png)

很容易看出qi > 0并且，Σi=(1,k) qi = 1,所有q是有效的概率分布。

另一方面，假设这个图像的真标签是y，我们可以构造一个真概率分布，当i = y时为pi = 1，否则为0。





> > 

## 

## 结论

介绍了训练深度卷积神经网络以提高模型精度的十几个技巧。

这些技巧引入了对模型架构、数据预处理、损失函数和学习率的微小处理。在ResNet-50、Inception-V和MobileNet上的实验结果表明，这些技巧持续地提高了模型准确性。并且这些tricks的组合使用能显著提高准确性。并在迁移学习方面又很强的优势。